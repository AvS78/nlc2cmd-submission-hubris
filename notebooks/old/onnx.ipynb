{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for converting models to onnx  \n",
    "(Obsolete as this is now also implemented in the main codebase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.convert_graph_to_onnx import convert\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "from onnxruntime_tools import optimizer\n",
    "\n",
    "from os import environ\n",
    "from psutil import cpu_count\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from src.data_utils import encode, decode\n",
    "\n",
    "environ[\"OMP_NUM_THREADS\"] = str(cpu_count(logical=True))\n",
    "environ[\"OMP_WAIT_POLICY\"] = 'ACTIVE'\n",
    "from onnxruntime import InferenceSession, SessionOptions, get_all_providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX opset version set to: 11\n",
      "Loading pipeline (model: output/nl2bash/07-31_14:57:12, tokenizer: gpt2-medium)\n",
      "Using framework PyTorch: 1.5.1\n",
      "Found input input_ids with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found input attention_mask with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_0 with shape: {0: 'batch', 1: 'sequence'}\n",
      "Found output output_1 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_2 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_3 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_4 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_5 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_6 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_7 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_8 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_9 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_10 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_11 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_12 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_13 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_14 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_15 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_16 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_17 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_18 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_19 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_20 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_21 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_22 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_23 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Found output output_24 with shape: {1: 'batch', 3: 'sequence'}\n",
      "Ensuring inputs are in correct order\n",
      "past is not present in the generated input list.\n",
      "Generated inputs order: ['input_ids']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_gpt2.py:149: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  w = w / (float(v.size(-1)) ** 0.5)\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/modeling_gpt2.py:151: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  mask = self.bias[:, :, ns - nd : ns, :ns]\n",
      "/opt/conda/lib/python3.7/site-packages/torch/onnx/utils.py:912: UserWarning: Provided key attention_mask for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\"Provided key {} for dynamic axes is not a valid input/output name\".format(key))\n"
     ]
    }
   ],
   "source": [
    "model_path = \"output/nl2bash/07-31_14:57:12\"\n",
    "model_name = \"gpt2-medium\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.to('cuda')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "convert(framework=\"pt\", model=model_path, tokenizer=model_name, output=\"onnx/gpt2.onnx\", opset=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = optimizer.optimize_model(\"onnx/gpt2.onnx\", model_type=model_name)\n",
    "#optimized_model.convert_model_float32_to_float16()\n",
    "optimized_model.save_model_to_file(\"onnx/opt.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_for_provider(model_path: str, provider: str) -> InferenceSession: \n",
    "    assert provider in get_all_providers(), f\"provider {provider} not found, {get_all_providers()}\"\n",
    "    # Few properties than might have an impact on performances (provided by MS)\n",
    "    options = SessionOptions()\n",
    "    options.intra_op_num_threads = 1\n",
    "    # Load the model as a graph and prepare the CPU backend \n",
    "    return InferenceSession(model_path, options, providers=[provider])\n",
    "\n",
    "onnx_model = create_model_for_provider(\"onnx/gpt2.onnx\", \"CUDAExecutionProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onnx_predict(query):\n",
    "    query = encode(query)\n",
    "    model_inputs = tokenizer.encode_plus(query, return_tensors=\"pt\")\n",
    "    inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items() if k=='input_ids'}\n",
    "\n",
    "    # Run the model (None = get all the outputs)\n",
    "    for _ in range(100):\n",
    "        #print(inputs_onnx)\n",
    "        logits = onnx_model.run(None, inputs_onnx)[0][:,-1,:]\n",
    "        logits = model.lm_head(torch.tensor(logits).to('cuda'))\n",
    "        log_probs = F.softmax(logits, dim=-1)\n",
    "        _, prev = torch.topk(log_probs, k=1)\n",
    "        token = prev.item()\n",
    "        if token == 198:\n",
    "            break\n",
    "        inputs_onnx['input_ids'] = np.atleast_2d(np.append(inputs_onnx['input_ids'], prev.cpu().numpy()))\n",
    "    \n",
    "    return decode(tokenizer, inputs_onnx['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find v_1 v_2 v_3 -type d -exec bash -c 'mv \"$1\" \"${1%.*}.txt\"' -- {} \\;\n",
      "CPU times: user 635 ms, sys: 36.6 ms, total: 671 ms\n",
      "Wall time: 669 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(onnx_predict('Rename \"file.txt\" in directories \"v_1\", \"v_2\", and \"v_3\" each to \"v_1.txt\", \"v_2.txt\", and \"v_3.txt\" respectively and print the conversion'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-06 s\n",
       "\n",
       "Total time: 0.695339 s\n",
       "File: <ipython-input-8-67c3c8d5d989>\n",
       "Function: onnx_predict at line 1\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     1                                           def onnx_predict(query):\n",
       "     2         1         13.0     13.0      0.0      query = encode(query)\n",
       "     3         1       2150.0   2150.0      0.3      model_inputs = tokenizer.encode_plus(query, return_tensors=\"pt\")\n",
       "     4         1         32.0     32.0      0.0      inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items() if k=='input_ids'}\n",
       "     5                                           \n",
       "     6                                               # Run the model (None = get all the outputs)\n",
       "     7        37        229.0      6.2      0.0      for _ in range(100):\n",
       "     8                                                   #print(inputs_onnx)\n",
       "     9        37     656478.0  17742.6     94.4          logits = onnx_model.run(None, inputs_onnx)[0][:,-1,:]\n",
       "    10        37      11171.0    301.9      1.6          logits = model.lm_head(torch.tensor(logits).to('cuda'))\n",
       "    11        37       2518.0     68.1      0.4          log_probs = F.softmax(logits, dim=-1)\n",
       "    12        37       1959.0     52.9      0.3          _, prev = torch.topk(log_probs, k=1)\n",
       "    13        37      13984.0    377.9      2.0          token = prev.item()\n",
       "    14        37        106.0      2.9      0.0          if token == 198:\n",
       "    15         1          3.0      3.0      0.0              break\n",
       "    16        36       5903.0    164.0      0.8          inputs_onnx['input_ids'] = np.atleast_2d(np.append(inputs_onnx['input_ids'], prev.cpu().numpy()))\n",
       "    17                                               \n",
       "    18         1        793.0    793.0      0.1      return decode(tokenizer, inputs_onnx['input_ids'][0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f onnx_predict onnx_predict('Rename \"file.txt\" in directories \"v_1\", \"v_2\", and \"v_3\" each to \"v_1.txt\", \"v_2.txt\", and \"v_3.txt\" respectively and print the conversion')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
