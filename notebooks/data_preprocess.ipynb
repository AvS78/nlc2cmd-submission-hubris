{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing notebook\n",
    "This notebook contains all code related to the pre-processing of raw data, into the format we used by the trainer\n",
    "The kind of pre-processing depends on which source. The man pages and scraped sites are used for pre-training, while nl2bash data, ainix data, mankier data and Stack overflow data are used for pairs. Lastly, al commands are converted to a template for.  \n",
    "In the man pages, the non-technical usefull sections are removed. For the stack overflow, a series of heuritics are used to filter bad questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting bashlex grammar using file: /home/jaron/shared/internship-jaron/bashlint/grammar/grammar100.txt\n",
      "Bashlint grammar set up (148 utilities)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import html\n",
    "import re\n",
    "import bs4\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.insert(0, module_path)\n",
    "\n",
    "import bashlint.bash as bash\n",
    "from bashlint.data_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Man pages\n",
    "source: https://github.com/IBM/clai/blob/nlc2cmd/docs/manpage-data.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36669 man pages loaded\n"
     ]
    }
   ],
   "source": [
    "with open('data/raw/manpage-data.json') as f:\n",
    "    mandump = f.readlines()\n",
    "    \n",
    "mandump = [json.loads(line) for line in mandump]\n",
    "print(len(mandump), \"man pages loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # remove bold and underscore\n",
    "    text = text.replace('<b>','').replace('</b>', '')\n",
    "    text = text.replace('<u>','').replace('</u>', '')\n",
    "    # remove links\n",
    "    text = re.sub(r'<a.*?>', '', text).replace('</a>', '')\n",
    "    # decode html escaping\n",
    "    text = html.unescape(text)\n",
    "    return text \n",
    "\n",
    "with open('data/raw/man_cleaned.txt', 'w+') as f:\n",
    "    for page in mandump:\n",
    "        content = page['paragraphs']\n",
    "\n",
    "        for paragraph in content:\n",
    "            section_name = paragraph['section']\n",
    "            if not section_name:\n",
    "                continue\n",
    "            section_name = section_name.lower().strip()\n",
    "            if 'examp' in section_name or 'flag' in section_name or 'option' in section_name:\n",
    "                text = clean_text(paragraph['text'])\n",
    "                # throw way the very long paragraphs\n",
    "                if len(text) < 800:\n",
    "                    print(text, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mankier doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/mankierdocs.json') as f:\n",
    "    mankier = json.load(f)\n",
    "\n",
    "def process_line(line):\n",
    "    line = line.strip(\"$ \")\n",
    "    if not line:\n",
    "        return None\n",
    "    # remove shebangs\n",
    "    if line.startswith('#!'):\n",
    "        return None\n",
    "    # remove comments\n",
    "    if line.startswith('#'):\n",
    "        return None\n",
    "    \n",
    "    parsed = bash_parser(line)\n",
    "    try:\n",
    "        template = ast2template(parsed)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    #if len(template) > 7 and len(template) != len(line):\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/mankier_nl.txt', 'w+') as f_nl:\n",
    "    with open('data/raw/mankier_cm.txt', 'w+') as f_cm:\n",
    "        for k, v in tuple(mankier.items()):\n",
    "            #print(k)\n",
    "            page = bs4.BeautifulSoup(v)\n",
    "            page = page.body.main\n",
    "            examples = page.find('section', id=\"Examples_(TL;DR)\")\n",
    "            if examples is not None:\n",
    "                for li in examples.find_all('li'):\n",
    "                    nl = li.span.get_text().strip(\" :\")\n",
    "                    cm = li.find_all('code')[-1].get_text()\n",
    "                    cm = process_line(cm)\n",
    "                    \n",
    "                    if cm is None:\n",
    "                        continue\n",
    "                    print(nl, file=f_nl)\n",
    "                    print(cm, file=f_cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AInix\n",
    "source: https://github.com/DNGros/ai-nix-kernal-dataset-archie-json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/ainix-kernal.json') as f:\n",
    "    ainix = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_templates(s):\n",
    "    # quick'n dirty replace of the template tokens used in AInix\n",
    "    # note: specific values in the cms are not important, \n",
    "    # as it will go through the templater later\n",
    "    s = s.replace('[-[ENGWORD]-]', 'word')\n",
    "    s = s.replace('[-[1=ENGWORD]-]', 'foo')\n",
    "    s = s.replace('[-[2=ENGWORD]-]', 'bar')\n",
    "    s = s.replace('[-[3=ENGWORD]-]', 'spam')\n",
    "    s = s.replace('[-[USERNAME]-]', 'username')\n",
    "    s = s.replace('[-[GROUPNAME]-]', 'groupname')\n",
    "    s = s.replace('[-[DIRNAME]-]', 'folder')\n",
    "    s = s.replace('[-[1=DIRNAME]-]', 'folder1')\n",
    "    s = s.replace('[-[2=DIRNAME]-]', 'folder2')\n",
    "    s = s.replace('[-[3=DIRNAME]-]', 'folder3')\n",
    "    s = s.replace('[-[FILENAME]-]', 'file.txt')\n",
    "    s = s.replace('[-[1=FILENAME]-]', 'file1.txt')\n",
    "    s = s.replace('[-[2=FILENAME]-]', 'file2.txt')\n",
    "    s = s.replace('[-[3=FILENAME]-]', 'file3.txt')\n",
    "    s = s.replace('[-[EXTENSION]-]', 'txt')\n",
    "    s = s.replace('[-[$1]-]', 'arg1')\n",
    "    s = s.replace('[-[$2]-]', 'arg2')\n",
    "    s = s.replace('[-[$3]-]', 'arg3')\n",
    "    return s\n",
    "\n",
    "with open('data/raw/ainix_nl.txt', 'w+') as f_nl:\n",
    "    with open('data/raw/ainix_cm.txt', 'w+') as f_cm:\n",
    "        for _, v in ainix.items():\n",
    "            nls = v['x']\n",
    "            cms = v['y']\n",
    "            \n",
    "            nls = [replace_templates(nl['x_text']) for nl in nls]\n",
    "            cms = [replace_templates(cm['y_text']) for cm in cms]\n",
    "            cms = [process_line(cm) for cm in cms]\n",
    "            cms = list(set([cm for cm in cms if cm]))\n",
    "            A, B = len(nls), len(cms)\n",
    "            random.shuffle(nls)\n",
    "            random.shuffle(cms)\n",
    "            for i in range(max(A, B)):\n",
    "                nl = nls[i%A]\n",
    "                cm = cms[i%B]\n",
    "                assert cm is not None\n",
    "                print(nl, file=f_nl)\n",
    "                print(cm, file=f_cm)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraped sites\n",
    "source: https://drive.google.com/file/d/1KmijhOXS9PI7TB0XWJ8E1g5eP2hLVlCp/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/scraped_sites.json') as f:\n",
    "    scrape = json.load(f)\n",
    "scrape = tuple(scrape.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/scrape_examples.txt', 'w+') as f:\n",
    "    for k, v in scrape:\n",
    "        #print(k)\n",
    "        page = bs4.BeautifulSoup(v).body.find('div', id='main-content')\n",
    "        if not page:\n",
    "            continue\n",
    "\n",
    "        nlp = page.find('h2', id='examples')\n",
    "        previous_nl = ''\n",
    "        while nlp is not None:\n",
    "            cmp = nlp.find_next_sibling('pre')\n",
    "            if cmp is None:\n",
    "                break\n",
    "\n",
    "            nlp = cmp.find_next_sibling('p')\n",
    "            cm = cmp.get_text().strip()\n",
    "            nl = nlp.get_text().strip()\n",
    "            if nl.startswith('Same as'):\n",
    "                nl = previous_nl\n",
    "            else:\n",
    "                previous_nl = nl\n",
    "            if '\\n' not in str(cm):\n",
    "                print(nl, file=f)\n",
    "                print(cm, file=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack-overflow dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103734 entries loaded\n"
     ]
    }
   ],
   "source": [
    "with open('data/raw/stackoverflow.com-bash-top5answers.score.0.json') as f:\n",
    "    dump = json.load(f)\n",
    "print(len(dump), 'entries loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64363 entries removed\n"
     ]
    }
   ],
   "source": [
    "# We want to remove questions that:\n",
    "# 1) ask for explanation/rationale about something\n",
    "# 2) ask for help with an error/issue\n",
    "\n",
    "illegal_words = {\n",
    "    # type 1\n",
    "    'understand',\n",
    "    'why',\n",
    "    'difference'\n",
    "    'explain',\n",
    "    'mean',\n",
    "    'how is',\n",
    "    'how are',\n",
    "    # type 2\n",
    "    'syntax error',\n",
    "    'error:',\n",
    "    'exception',\n",
    "    'fail',\n",
    "    'crash',\n",
    "    'issue',\n",
    "    'problem',\n",
    "    'expected',\n",
    "    'invalid',\n",
    "    'bad',\n",
    "    'wrong',\n",
    "    \"n't work\",\n",
    "    'not work',\n",
    "    \"won't\",\n",
    "    \"can't\",\n",
    "    'always',\n",
    "    ' fix ', # avoid matching e.g. suffix\n",
    "    'solve',\n",
    "    'not found',\n",
    "    'help me',\n",
    "    'stuck',\n",
    "    'cause',\n",
    "    'throws',\n",
    "    'denied',\n",
    "    'messed',\n",
    "}\n",
    "\n",
    "\n",
    "cdump = []\n",
    "for entry in dump:\n",
    "    title = entry['title'].lower()\n",
    "    body = entry['body'].lower()\n",
    "    illegal = False\n",
    "    for iword in illegal_words:\n",
    "        if iword in title or iword in body:\n",
    "            illegal = True\n",
    "    if len(title)<18:\n",
    "        illegal=True\n",
    "        \n",
    "    if not illegal:\n",
    "        entry['title'] = entry['title'].strip(\" ?\\n\\t\")\n",
    "        cdump.append(entry)\n",
    "\n",
    "print(len(dump)-len(cdump), 'entries removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant prefies from the titles\n",
    "redundant_prefixes = {\n",
    "    'shell:',\n",
    "    'bash:',\n",
    "    'in bash',\n",
    "    'how do i',\n",
    "    'how do you',\n",
    "    'how to',\n",
    "    'how can i',\n",
    "    'how can you',\n",
    "    'trying to',\n",
    "    'best way to',\n",
    "    'a way to',\n",
    "    'is there',\n",
    "    'is it possible to'\n",
    "}\n",
    "for entry in cdump:\n",
    "    title = entry['title'].lower()\n",
    "    for prefix in redundant_prefixes:\n",
    "        if title.startswith(prefix):\n",
    "            entry['title'] = entry['title'][len(prefix):].strip(\" \").capitalize()\n",
    "            title = entry['title'].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10028 entries removed\n",
      "29343 entries remaining\n"
     ]
    }
   ],
   "source": [
    "# remove questions that don't contain any code in their answers\n",
    "# also throws away answers with score <= 0\n",
    "\n",
    "def contains_snippet(s):\n",
    "    return s.find(\"<pre><code>\") != -1\n",
    "\n",
    "ccdump = []\n",
    "for entry in cdump:\n",
    "    entry['answers'] = [x for x in entry['answers']\n",
    "                       if contains_snippet(x['body']) and int(x['score']) > 0]\n",
    "        \n",
    "    if len(entry['answers']):\n",
    "        ccdump.append(entry)\n",
    "        \n",
    "print(len(cdump)-len(ccdump), 'entries removed')\n",
    "print(len(ccdump), \"entries remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    line = line.strip(\"$ \")\n",
    "    if not line:\n",
    "        return None\n",
    "    # remove shebangs\n",
    "    if line.startswith('#!'):\n",
    "        return None\n",
    "    # remove comments\n",
    "    if line.startswith('#'):\n",
    "        return None\n",
    "    \n",
    "    parsed = bash_parser(line)\n",
    "    try:\n",
    "        template = ast2template(parsed)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    if len(template) > 7 and len(template) != len(line):\n",
    "        return template\n",
    "\n",
    "\n",
    "def get_snippets(answer):\n",
    "    answer = bs4.BeautifulSoup(answer)\n",
    "    # doesn't include inline code\n",
    "    snippets = answer.find_all('pre')\n",
    "    r = set()\n",
    "    for snip in snippets:\n",
    "        snip = snip.find('code')\n",
    "        if not snip:\n",
    "            continue\n",
    "        snip = snip.text.strip()\n",
    "        lsnip = snip.lower()\n",
    "        \n",
    "        # no long snippets\n",
    "        if lsnip.count('\\n')>4:\n",
    "            continue\n",
    "        \n",
    "        # no if statements\n",
    "        if 'if' in lsnip and 'then' in lsnip:\n",
    "            continue\n",
    "        # no while statements\n",
    "        if 'while ' in lsnip and 'done' in lsnip:\n",
    "            continue\n",
    "        # no for loops\n",
    "        if 'for ' in lsnip and 'done' in lsnip:\n",
    "            continue\n",
    "        # no function defenitions\n",
    "        if ('()' in lsnip or 'function' in lsnip) and '{' in lsnip and '}' in lsnip:\n",
    "            continue\n",
    "        \n",
    "        for line in snip.split('\\n'):\n",
    "            line = process_line(line)\n",
    "            if line is not None:\n",
    "                r.add(line)\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/ST_nl.txt', 'w+') as f_nl:\n",
    "    with open('data/raw/ST_cm.txt', 'w+') as f_cm:\n",
    "        for entry in ccdump:\n",
    "            snippets = set()\n",
    "            for answer in entry['answers']:\n",
    "                snippets |= get_snippets(answer['body'])\n",
    "            for s in tuple(snippets)[:5]:\n",
    "                print(entry['title'], file=f_nl)\n",
    "                print(s, file=f_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NL2Bash / Tellina\n",
    "source: https://github.com/IBM/clai/blob/nlc2cmd/docs/nl2bash-data.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/raw/nl2bash-data.json\") as f:\n",
    "    data = json.load(f)\n",
    "    data = [x[1] for x in data.items()]\n",
    "    \n",
    "nls = [x['invocation'] for x in data]\n",
    "cms = [x['cmd'] for x in data]\n",
    "\n",
    "with open('data/raw/nl2bash_nl.txt', 'w+') as f_nl:\n",
    "    with open('data/raw/nl2bash_cm.txt', 'w+') as f_cm:\n",
    "        for nl, cm in zip(nls, cms):\n",
    "            parsed = bash_parser(cm)\n",
    "            try:\n",
    "                template = ast2template(parsed)\n",
    "            except:\n",
    "                continue\n",
    "            print(nl, file=f_nl)\n",
    "            print(template, file=f_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together\n",
    "The data is combined and divided into train/val/test splits.\n",
    "Note that the construction of test/val is to be used to check e.g. overfitting and NOT to get a objective result on the accuracy! There's NO check for overlap between test-train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how large test/val splits?\n",
    "test_size = 200\n",
    "val_size = 200\n",
    "\n",
    "with open('data/raw/nl2bash_nl.txt') as f:\n",
    "    nls = f.readlines()\n",
    "with open('data/raw/nl2bash_cm.txt') as f:\n",
    "    cms = f.readlines()\n",
    "\n",
    "with open('data/raw/ainix_nl.txt') as f:\n",
    "    nls += f.readlines()\n",
    "with open('data/raw/ainix_cm.txt') as f:\n",
    "    cms += f.readlines()\n",
    "   \n",
    "with open('data/raw/mankier_nl.txt') as f:\n",
    "    nls += f.readlines()\n",
    "with open('data/raw/mankier_cm.txt') as f:\n",
    "    cms += f.readlines()\n",
    "    \n",
    "pairs = list(zip(nls, cms))\n",
    "random.shuffle(pairs)\n",
    "\n",
    "test_pairs = pairs[:test_size]\n",
    "pairs = pairs[test_size:]\n",
    "\n",
    "val_pairs = pairs[:val_size]\n",
    "pairs = pairs[val_size:]\n",
    "\n",
    "train_pairs = pairs\n",
    "\n",
    "def write_pairs(pairs, name):\n",
    "    nls = [x[0] for x in pairs]\n",
    "    cms = [x[1] for x in pairs]\n",
    "    with open('data/clai/'+name+'_nl.txt', 'w+') as f:\n",
    "        f.write(''.join(nls))\n",
    "    with open('data/clai/'+name+'_cm.txt', 'w+') as f:\n",
    "        f.write(''.join(cms))        \n",
    "        \n",
    "write_pairs(test_pairs, 'test')\n",
    "write_pairs(val_pairs, 'dev')\n",
    "write_pairs(train_pairs, 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/ST_nl.txt') as f:\n",
    "    nls = f.readlines()\n",
    "with open('data/raw/ST_cm.txt') as f:\n",
    "    cms = f.readlines()\n",
    "    \n",
    "pairs = list(zip(nls, cms))\n",
    "write_pairs(pairs, 'dirty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/man_cleaned.txt') as f:\n",
    "    content = f.read()\n",
    "with open('data/raw/scrape_examples.txt') as f:\n",
    "    content += f.read()\n",
    "    \n",
    "with open('data/clai/pre.txt', 'w+') as f:\n",
    "    f.write(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
